{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"<ADD_API_KEY_HERE>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASPIRE_LINQX.core.library.base import BaseDriverMicroservice\n",
    "from ASPIRE_LINQX.ai.agents.core import command_to_tool, create_linqx_chat_agent\n",
    "from ASPIRE_LINQX.ai.chains.microservice import module_to_microservice\n",
    "from ASPIRE_LINQX.testing.models.drivers import MicrowaveSynthesizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Based Benchmarking\n",
    "\n",
    "### Basic Agent Output Benchmarking\n",
    "\n",
    "Output based benchmarking compares the output(s) of the agent to the desired output(s)\n",
    "\n",
    "Required:\n",
    "- Function which returns an Agent Executor\n",
    "- Keyword arguments to pass to that function to create an agent to desired specifications\n",
    "- Initial input to benchmark the agent on\n",
    "- Output string(s) to direct match with the agents output(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_command_microservice = module_to_microservice(MicrowaveSynthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from ASPIRE_LINQX.utils.benchmarking.output import AgentOutputBenchmarker\n",
    "\n",
    "executor_kwargs = {\n",
    "    'microservice': driver_command_microservice,\n",
    "    'use_pubchem': False,\n",
    "    'llm': ChatOpenAI(model='gpt-4'),\n",
    "    'human_interaction': False,\n",
    "    'agent_as_a_tool': None,\n",
    "    'use_linqx_tools': True,\n",
    "}\n",
    "\n",
    "output_benchmark = AgentOutputBenchmarker(\n",
    "    executor_fn=create_linqx_chat_agent,\n",
    "    executor_kwargs=executor_kwargs,\n",
    "    initial_input=\"Heat vial 3 to 100 degrees, for 50 mins, at 3 atm. Return you final answer as 'vial number X' or 'X' where X in the vial loaded\",\n",
    "    verbose=True,\n",
    "    notebook=True,\n",
    "    desired_output=['vial number 3', '3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_benchmark.benchmark(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegEx Based Output Benchmarking\n",
    "\n",
    "Checks the agent output for matches with one or more regular expression(s)\n",
    "\n",
    "Required:\n",
    "- Function which returns an Agent Executor\n",
    "- Keyword arguments to pass to that function to create an agent to desired specifications\n",
    "- Initial input to benchmark the agent on\n",
    "- Regular expression(s) to attempt to match the agent output(s) with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from ASPIRE_LINQX.utils.benchmarking.output import AgentRegexOutputBenchmarker\n",
    "\n",
    "executor_kwargs = {\n",
    "    'microservice': driver_command_microservice,\n",
    "    'use_pubchem': False,\n",
    "    'llm': ChatOpenAI(model='gpt-4'),\n",
    "    'human_interaction': False,\n",
    "    'agent_as_a_tool': None,\n",
    "    'use_linqx_tools': True,\n",
    "}\n",
    "\n",
    "regex_output_benchmark = AgentRegexOutputBenchmarker(\n",
    "    executor_fn=create_linqx_chat_agent,\n",
    "    executor_kwargs=executor_kwargs,\n",
    "    initial_input=\"Heat vial 3 to 100 degrees, for 50 mins, at 3 atm. Include in your final answer 'vial number X' where X in the vial loaded\",\n",
    "    verbose=True,\n",
    "    notebook=True,\n",
    "    desired_output=r'(V|v)ial number 3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_output_benchmark.benchmark(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Based Output Benchmarking (Agent may have issues with output formatting)\n",
    "\n",
    "Validates the agent output (JSON) against Pydantic validation schemas\n",
    "\n",
    "Required:\n",
    "- Function which returns an Agent Executor\n",
    "- Keyword arguments to pass to that function to create an agent to desired specifications\n",
    "- Initial input to benchmark the agent on\n",
    "- Pydantic validation schema(s) to run on the agent output(s) \n",
    "    - Note: Agent output must be a dictionary or a JSON formatted string which can be loaded to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from ASPIRE_LINQX.utils.benchmarking.output import AgentJsonOutputBenchmarker\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "class VialSchema(BaseModel):\n",
    "    vial_number: Literal[3, '3']\n",
    "\n",
    "executor_kwargs = {\n",
    "    'microservice': driver_command_microservice,\n",
    "    'use_pubchem': False,\n",
    "    'llm': ChatOpenAI(model='gpt-4'),\n",
    "    'human_interaction': False,\n",
    "    'agent_as_a_tool': None,\n",
    "    'use_linqx_tools': True,\n",
    "}\n",
    "\n",
    "json_output_benchmark = AgentJsonOutputBenchmarker(\n",
    "    executor_fn=create_linqx_chat_agent,\n",
    "    executor_kwargs=executor_kwargs,\n",
    "    initial_input=\"Heat vial 3 to 100 degrees, for 50 mins, at 3 atm. Return your final answer as a JSON formatted string with a key of 'vial_number' and value of the vial loaded\",\n",
    "    verbose=True,\n",
    "    notebook=True,\n",
    "    desired_output=VialSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output_benchmark.benchmark(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Based Benchmarking\n",
    "\n",
    "Path based benchmarking compares the agent action path to one or more desired paths of operation\n",
    "\n",
    "Required:\n",
    "- Function which returns an Agent Executor\n",
    "- Keyword arguments to pass to that function to create an agent to desired specifications\n",
    "- Initial input to benchmark the agent on\n",
    "- List(s) of action command names or (name, Pydantic schema for command input) tuples which represent the desired agent action path(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_command_microservice = module_to_microservice(MicrowaveSynthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from ASPIRE_LINQX.utils.benchmarking.path import AgentPathBenchmarker\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class LoadVialSchema(BaseModel):\n",
    "    vial_num: Literal[3]\n",
    "    session_ID: str\n",
    "\n",
    "class HeatingParameterSchema(BaseModel):\n",
    "    duration: Literal[50]\n",
    "    temperature: Literal[100]\n",
    "    pressure: Literal[3]\n",
    "    session_ID: str\n",
    "\n",
    "path = [\n",
    "    'allocate_session',\n",
    "    'open_lid',\n",
    "    ('load_vial', LoadVialSchema),\n",
    "    'close_lid',\n",
    "    ('update_heating_parameters', HeatingParameterSchema),\n",
    "    'heat_vial'\n",
    "]\n",
    "\n",
    "path_2 = [\n",
    "    'allocate_session',\n",
    "    'open_lid',\n",
    "    ('load_vial', LoadVialSchema),\n",
    "    ('update_heating_parameters', HeatingParameterSchema),\n",
    "    'heat_vial',\n",
    "]\n",
    "\n",
    "executor_kwargs = {\n",
    "    'microservice': driver_command_microservice,\n",
    "    'use_pubchem': False,\n",
    "    'llm': ChatOpenAI(model='gpt-4'),\n",
    "    'human_interaction': False,\n",
    "    'agent_as_a_tool': None,\n",
    "    'use_linqx_tools': True,\n",
    "}\n",
    "\n",
    "path_benchmark = AgentPathBenchmarker(\n",
    "    executor_fn=create_linqx_chat_agent,\n",
    "    executor_kwargs=executor_kwargs,\n",
    "    initial_input='Heat vial 3 to 100 degrees, for 50 mins, at 3 atm',\n",
    "    desired_output=[path_2, path],\n",
    "    verbose=True,\n",
    "    notebook=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_benchmark.benchmark(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Based Benchmarking\n",
    "\n",
    "State based benchmarking compares the final state of the system after agent operation to the desired final state\n",
    "\n",
    "Required:\n",
    "- Function which returns an Agent Executor\n",
    "- Keyword arguments to pass to that function to create an agent to desired specifications\n",
    "- Initial input to benchmark the agent on\n",
    "- Pydantic schema of the systems allowed initial state(s)\n",
    "- Pydantic schema of the systems desired final states(s)\n",
    "- Function which can access the state of the system at any given time\n",
    "\n",
    "Optional:\n",
    "- Function which resets the system state\n",
    "- User confirmation that the systems state has been reset (external to code endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASPIRE_LINQX.ai.chains.microservice import object_to_microservice\n",
    "from ASPIRE_LINQX.testing.models.drivers.MicrowaveSynthesizerObject import MicrowaveSynthesizer as MSObject\n",
    "\n",
    "ms_object = MSObject()\n",
    "object_driver_microservice = object_to_microservice(object=ms_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class IntialState(BaseModel):\n",
    "    sessionID: None\n",
    "    lid_status: Literal['closed']\n",
    "    vial_status: Literal['unloaded']\n",
    "    vial_number: None\n",
    "    heating_status: Literal['not_heating']\n",
    "    temp: None\n",
    "    duration: None\n",
    "    pressure: None\n",
    "\n",
    "class FinalState(BaseModel):\n",
    "    sessionID: str\n",
    "    lid_status: Literal['closed']\n",
    "    vial_status: Literal['loaded']\n",
    "    vial_number: Literal[3]\n",
    "    heating_status: Literal['heating']\n",
    "    temp: Literal[100]\n",
    "    duration: Literal[50]\n",
    "    pressure: float = Field(ge=3.0, lt=3.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASPIRE_LINQX.utils.benchmarking.state import AgentStateBenchmarker\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "executor_kwargs = {\n",
    "    'microservice': object_driver_microservice,\n",
    "    'use_pubchem': False,\n",
    "    'llm': ChatOpenAI(model='gpt-4'),\n",
    "    'human_interaction': False,\n",
    "    'agent_as_a_tool': None,\n",
    "    'use_linqx_tools': True,\n",
    "}\n",
    "\n",
    "state_benchmarker = AgentStateBenchmarker(\n",
    "    executor_fn=create_linqx_chat_agent,\n",
    "    executor_kwargs=executor_kwargs,\n",
    "    initial_input='Heat vial 3 to 100 degrees, for 50 mins, at 3 atm',\n",
    "    verbose=True,\n",
    "    notebook=True,\n",
    "    initial_state=IntialState,\n",
    "    desired_output=[FinalState],\n",
    "    current_state=ms_object.dict,\n",
    "    reset_system=ms_object._reset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_benchmarker.benchmark(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
